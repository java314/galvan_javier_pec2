# -*- coding: utf-8 -*-
"""ejercicio_1_pec_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yw5vXqsvGXfcVx46T2g-yFXjT4gId2Ri
"""

import numpy as np
import pandas as pd

from sklearn.linear_model import Ridge

from sklearn.metrics import mean_squared_error, mean_absolute_error

from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, GridSearchCV

from sklearn.pipeline import Pipeline

from sklearn.compose import TransformedTargetRegressor

from sklearn import preprocessing

from sklearn.preprocessing import OneHotEncoder

import joblib

import matplotlib as plt

import seaborn as sns

from google.colab import files

uploaded = files.upload()

# Luego, selecciona el archivo utilizando el botón de carga y luego ejecuta:

import pandas as pd

df_casas = pd.read_csv(next(iter(uploaded.keys())))

df_casas.head()

sns.pairplot(df_casas[["PriceOfUnitArea"]])

sns.pairplot(np.log10(df_casas[["PriceOfUnitArea"]]))

df_casas.info()

# Convertir la columna "TransactionDate" a tipo de dato fecha
df_casas['TransactionDate'] = pd.to_datetime(df_casas['TransactionDate'])

# Extraer características numéricas de la fecha
df_casas['Year'] = df_casas['TransactionDate'].dt.year
df_casas['Month'] = df_casas['TransactionDate'].dt.month
df_casas['DayOfWeek'] = df_casas['TransactionDate'].dt.dayofweek

# Eliminar la columna original "TransactionDate" si ya no es necesaria
df_casas = df_casas.drop('TransactionDate', axis=1)

# defino los limites
bins = [0, 500, 1454, float('inf')]
labels = ['Cercano', 'Medio', 'Lejano']

# convierto a categorica
df_casas['DistanceToMRT_Category'] = pd.cut(df_casas['DistanceToMRT'], bins=bins, labels=labels)

print(df_casas)

x,y = df_casas.drop(["PriceOfUnitArea", "No"], axis =1), df_casas["PriceOfUnitArea"]





df_casas.head()



summary_stats = df_casas['DistanceToMRT'].describe()
print(summary_stats)



df_casas.info()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder

categorical_columns = ["DistanceToMRT_Category"]
numerical_columns = ["HouseAge", "NumberConvenienceStores", "Latitude", "Longitude", "Year"]

preprocessor = make_column_transformer((OneHotEncoder(drop="if_binary"), categorical_columns),
                                       remainder="passthrough",
                                       verbose_feature_names_out=False,)

from sklearn.compose import TransformedTargetRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import numpy as np
import scipy as sp


model = make_pipeline(
    preprocessor,
    TransformedTargetRegressor(
        regressor=Ridge(alpha=1e-10),
        func=np.log10,
        inverse_func=sp.special.exp10
    ),
)



X_train

y_train

model.fit(X_train, y_train)

from sklearn.metrics import median_absolute_error

y_pred = model.predict(X_train)

mae = median_absolute_error(y_train, y_pred)
string_score = f"MAE on training set: {mae: 2f} usd"
y_pred = model.predict(X_test)
mae = median_absolute_error(y_test, y_pred)
string_score += f"\nMAR on testing set: {mae: 2f} usd"

import matplotlib.pyplot as plt

fig, ax = plt.subplots (figsize = (5,5))
plt.scatter(y_test, y_pred)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c="red")
plt.title("Ridge model, small regularization")
plt.ylabel("Model predictions")
plt.xlabel("verdad")

feature_names = model[:-1].get_feature_names_out()

coefs = pd.DataFrame(
    model[-1].regressor_.coef_,
    columns=["Coefficients"],
    index=feature_names
)

coefs

coefs.plot.barh(figsize=(9,7))
plt.title("Ridge model, small regularization")
plt.axvline(x=0, color=".5")
plt.xlabel("Raw coefficient values")
plt.subplots_adjust(left=0.3)

X_train_preprocessed = pd.DataFrame(
    model[:-1].transform(X_train), columns=feature_names
)

X_train_preprocessed.std(axis=0).plot.barh(figsize=(9,7))
plt.title("Feature ranges")
plt.xlabel("Std. dev. of feature values")
plt.subplots_adjust(left=0.3)

coefs = pd.DataFrame(
    model[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),
    columns=["Coefficient importance"],
    index=feature_names,
)
coefs.plot(kind="barh", figsize=(9,7))
plt.xlabel("Coefficient values corrected by the feature's std. dev.")
plt.title("Ridge model, small regularization")
plt.axvline(x=0, color=".5")
plt.subplots_adjust(left=0.3)

from sklearn.model_selection import RepeatedKFold, cross_validate

cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)
cv_model = cross_validate(
    model,
    x,
    y,
    cv=cv,
    return_estimator=True,
    n_jobs=2,
)
coefs = pd.DataFrame(
    [
        est[-1].regressor_.coef_
        for est, (train_idx, _) in zip(cv_model["estimator"], cv.split(x, y))
    ],
    columns=feature_names,
)



plt.figure(figsize=(5, 5))
sns.stripplot(data=coefs, orient="h", color="k", alpha=0.5)
sns.boxplot(data=coefs, orient="h", color="cyan", saturation=0.5)
plt.axvline(x=0, color="0.5")
plt.title("Coeficiente de importancia y su variabilidad")
plt.xlabel("Coeficiente de importancia")
plt.show()

"""Este resultado puede interpretarse como que la ubicación geográfica (representada por la latitud y longitud) tiene un impacto significativo en el precio de la unidad de área. Es posible que las propiedades ubicadas en ciertas áreas geográficas tengan valores de precios más altos debido a factores como la accesibilidad, servicios cercanos, entorno, entre otros."""

from sklearn.linear_model import RidgeCV

alphas = np.logspace(-10, 10 , 21)
model = make_pipeline(
    preprocessor,
    TransformedTargetRegressor(
        regressor=RidgeCV(alphas=alphas),
        func=np.log10,
        inverse_func=sp.special.exp10,
    ),
)
model.fit(X_train, y_train)

model[-1].regressor_.alpha_

y_pred = model.predict(X_train)
mae = median_absolute_error(y_train, y_pred)
string_score = f"MAE: {mae:.2f}"
y_pred = model.predict(X_test)
mae = median_absolute_error(y_test,y_pred)
string_score = f"MAE: {mae:.2f}"

fig, ax = plt.subplots(figsize=(5, 5))  # Reducir el tamaño de la figura
plt.scatter(y_test, y_pred)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c="red")
plt.title("Ridge model, small regularization")
plt.ylabel("model prediction")
plt.xlabel("Truths")

"""Ejercicio 2"""

import pickle

with open ("modelo.pkl", "wb") as file:
  pickle.dump(model, file)

"""carga del modelo desde el archivo plk"""

with open ("modelo.pkl", "rb") as file:
  model =pickle.load(file)

"""Ejercicio 3"""

from flask import Flask, request, jsonify
import pickle

# Cargo el modelo entrenado desde el archivo pkl
with open('modelo.pkl', 'rb') as file:
    model = pickle.load(file)

# Incio la aplicación Flask
app = Flask(__name__)

# Definir la ruta y el método para realizar predicciones
@app.route('/predict', methods=['POST'])
def predict():
    # Obtener los datos de la solicitud
    data = request.json

    # Realizar la predicción utilizando el modelo cargado
    prediction = model.predict([data['features']])

    # Devolver la predicción como respuesta en formato JSON
    response = {'prediction': prediction.tolist()}
    return jsonify(response)

# Especificar la configuración para ejecutar la aplicación en todas las interfaces de red
if __name__ == '__main__':
    app.run()









"""Extra"""

!pip install --upgrade pip
!pip install tox

!tox

